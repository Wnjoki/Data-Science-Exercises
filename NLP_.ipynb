{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wnjoki/Data-Science-Exercises/blob/main/NLP_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAmvv1OSXSyA",
        "outputId": "2f4482b4-8911-4718-be7b-757fa96e22ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import gensim\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0HNj67byXSyF"
      },
      "outputs": [],
      "source": [
        "#Got a rewiew from amazon\n",
        "\n",
        "document = \"I ordered over £1000 worth of computer equipment on Prime for next day delivery. It never turned up and they claimed I had rejected the delivery. After a week of hassle with their customer service and two further failed deliveries I told them to stick their measly £10 gesture of good will and cancelled the order. I won't be using them again. There are plenty of suppliers who may charge a bit more, but at least they deliver, literally and metaphorically. There goes several hours of my life I'll never get back.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VozIOV4aXSyG"
      },
      "outputs": [],
      "source": [
        "# Tokenization using NLTK\n",
        "tokens = word_tokenize(\"document\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Yf5ElrS3XSyH"
      },
      "outputs": [],
      "source": [
        "# Stemming using NLTK\n",
        "ps = PorterStemmer()\n",
        "stemmed_tokens = [ps.stem(token) for token in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zTFXweGiXSyJ"
      },
      "outputs": [],
      "source": [
        "# Lemmatization using NLTK\n",
        "wnl = WordNetLemmatizer()\n",
        "lemmatized_tokens = [wnl.lemmatize(token) for token in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2sTb0YNXSyK",
        "outputId": "3f0bde7f-95a1-4274-e3f8-190b3a44a456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#Stop word removal using NLTK\n",
        "nltk.download('stopwords')\n",
        "nltk_stopwords = set(stopwords.words('english'))\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in nltk_stopwords]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_ob3D2akXSyL"
      },
      "outputs": [],
      "source": [
        "# Tokenization using Genism\n",
        "genism_tokens = gensim.utils.simple_preprocess(\"document\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bIheWw3yXSyL"
      },
      "outputs": [],
      "source": [
        "# Tokenization using Spacy\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "spacy_doc = nlp(document)\n",
        "spacy_tokens = [token.text for token in spacy_doc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jyz47H-1XSyN"
      },
      "outputs": [],
      "source": [
        "# Lemmatization using Spacy\n",
        "spacy_lemmatized_tokens = [token.lemma_ for token in spacy_doc]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxsXRMYsXSyO",
        "outputId": "e51929a0-02e3-4377-a188-8208a841217f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Tokens: ['document']\n",
            "NLTK Stemmed Tokens: ['document']\n",
            "NLTK Lemmatized Tokens: ['document']\n",
            "NLTK Filtered Tokens: ['document']\n",
            "Spacy Tokens: ['I', 'ordered', 'over', '£', '1000', 'worth', 'of', 'computer', 'equipment', 'on', 'Prime', 'for', 'next', 'day', 'delivery', '.', 'It', 'never', 'turned', 'up', 'and', 'they', 'claimed', 'I', 'had', 'rejected', 'the', 'delivery', '.', 'After', 'a', 'week', 'of', 'hassle', 'with', 'their', 'customer', 'service', 'and', 'two', 'further', 'failed', 'deliveries', 'I', 'told', 'them', 'to', 'stick', 'their', 'measly', '£', '10', 'gesture', 'of', 'good', 'will', 'and', 'cancelled', 'the', 'order', '.', 'I', 'wo', \"n't\", 'be', 'using', 'them', 'again', '.', 'There', 'are', 'plenty', 'of', 'suppliers', 'who', 'may', 'charge', 'a', 'bit', 'more', ',', 'but', 'at', 'least', 'they', 'deliver', ',', 'literally', 'and', 'metaphorically', '.', 'There', 'goes', 'several', 'hours', 'of', 'my', 'life', 'I', \"'ll\", 'never', 'get', 'back', '.']\n",
            "Spacy Lemmatized Tokens: ['I', 'order', 'over', '£', '1000', 'worth', 'of', 'computer', 'equipment', 'on', 'Prime', 'for', 'next', 'day', 'delivery', '.', 'it', 'never', 'turn', 'up', 'and', 'they', 'claim', 'I', 'have', 'reject', 'the', 'delivery', '.', 'after', 'a', 'week', 'of', 'hassle', 'with', 'their', 'customer', 'service', 'and', 'two', 'further', 'fail', 'delivery', 'I', 'tell', 'they', 'to', 'stick', 'their', 'measly', '£', '10', 'gesture', 'of', 'good', 'will', 'and', 'cancel', 'the', 'order', '.', 'I', 'will', 'not', 'be', 'use', 'they', 'again', '.', 'there', 'be', 'plenty', 'of', 'supplier', 'who', 'may', 'charge', 'a', 'bit', 'more', ',', 'but', 'at', 'least', 'they', 'deliver', ',', 'literally', 'and', 'metaphorically', '.', 'there', 'go', 'several', 'hour', 'of', 'my', 'life', 'I', 'will', 'never', 'get', 'back', '.']\n",
            "Genism Tokens: ['document']\n"
          ]
        }
      ],
      "source": [
        "# Compare the output\n",
        "print(f\"NLTK Tokens: {tokens}\")\n",
        "print(f\"NLTK Stemmed Tokens: {stemmed_tokens}\")\n",
        "print(f\"NLTK Lemmatized Tokens: {lemmatized_tokens}\")\n",
        "print(f\"NLTK Filtered Tokens: {filtered_tokens}\")\n",
        "print(f\"Spacy Tokens: {spacy_tokens}\")\n",
        "print(f\"Spacy Lemmatized Tokens: {spacy_lemmatized_tokens}\")\n",
        "print(f\"Genism Tokens: {genism_tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Explanation of each step:\n",
        "\n",
        "#a. Tokenization: The process of splitting a text into individual words or tokens.\n",
        "\n",
        "#b. Stemming: The process of reducing words to their root or base form.\n",
        "\n",
        "#c. Lemmatization: The process of reducing words to their base or dictionary form, while taking into account the part of speech of the word.\n",
        "\n",
        "#d. Stop word removal: The process of removing common words that do not carry much meaning, such as \"the\", \"and\", \"a\", etc.\n",
        "\n",
        "\n",
        " ##The role of each library is as follows:\n",
        "# - NLTK: NLTK is a comprehensive library for natural language processing in Python. It provides a wide range of tools for tasks such as tokenization, stemming, lemmatization, and stop word removal.\n",
        "\n",
        "#- Spacy: Spacy is a fast and efficient library for natural language processing in Python. It is designed to be very fast and easy to use, with a focus on performance and production use cases.\n",
        "\n",
        "#- Genism: Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. It is also used for text preprocessing, including tokenization.\n"
      ],
      "metadata": {
        "id": "yPCu9gxgYp1A"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyzinng the document using bag of words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "document = [\"I ordered over £1000 worth of computer equipment on Prime for next day delivery.\",\n",
        "\" It never turned up and they claimed I had rejected the delivery.\",\n",
        " \"After a week of hassle with their customer service and two further failed deliveries I told them to stick their measly £10 gesture of good will and cancelled the order.\",\n",
        " \" I won't be using them again.\",\n",
        "  \"There are plenty of suppliers who may charge a bit more, but at least they deliver, literally and metaphorically.\",\n",
        "   \"There goes several hours of my life I'll never get back.\"]\n"
      ],
      "metadata": {
        "id": "nvUvW39HZZV3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a CountVectorizer object\n",
        "vectorizer = CountVectorizer(stop_words = 'english')\n",
        "\n",
        "# Fit and transform the document\n",
        "\n",
        "bow_model = vectorizer.fit_transform(document)\n",
        "print(bow_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_YdQt49YzI4",
        "outputId": "5108705f-855b-4890-d03c-5a97c91842c8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 25)\t1\n",
            "  (0, 1)\t1\n",
            "  (0, 37)\t1\n",
            "  (0, 6)\t1\n",
            "  (0, 12)\t1\n",
            "  (0, 27)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 11)\t1\n",
            "  (1, 11)\t1\n",
            "  (1, 33)\t1\n",
            "  (1, 5)\t1\n",
            "  (1, 28)\t1\n",
            "  (2, 35)\t1\n",
            "  (2, 17)\t1\n",
            "  (2, 7)\t1\n",
            "  (2, 29)\t1\n",
            "  (2, 13)\t1\n",
            "  (2, 10)\t1\n",
            "  (2, 32)\t1\n",
            "  (2, 30)\t1\n",
            "  (2, 22)\t1\n",
            "  (2, 0)\t1\n",
            "  (2, 14)\t1\n",
            "  (2, 16)\t1\n",
            "  (2, 3)\t1\n",
            "  (2, 24)\t1\n",
            "  (3, 36)\t1\n",
            "  (3, 34)\t1\n",
            "  (4, 26)\t1\n",
            "  (4, 31)\t1\n",
            "  (4, 4)\t1\n",
            "  (4, 2)\t1\n",
            "  (4, 9)\t1\n",
            "  (4, 20)\t1\n",
            "  (4, 23)\t1\n",
            "  (5, 15)\t1\n",
            "  (5, 18)\t1\n",
            "  (5, 19)\t1\n",
            "  (5, 21)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the sparse matrix to a Pandas DataFrame\n",
        "df = pd.DataFrame(bow_model.toarray(), columns=vectorizer.get_feature_names_out(), index=document)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgQp4WtedDu5",
        "outputId": "508d2e8b-79eb-4704-d0b4-ec851d5b2ff4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    10  1000  bit  cancelled  \\\n",
            "I ordered over £1000 worth of computer equipmen...   0     1    0          0   \n",
            " It never turned up and they claimed I had reje...   0     0    0          0   \n",
            "After a week of hassle with their customer serv...   1     0    0          1   \n",
            " I won't be using them again.                        0     0    0          0   \n",
            "There are plenty of suppliers who may charge a ...   0     0    1          0   \n",
            "There goes several hours of my life I'll never ...   0     0    0          0   \n",
            "\n",
            "                                                    charge  claimed  computer  \\\n",
            "I ordered over £1000 worth of computer equipmen...       0        0         1   \n",
            " It never turned up and they claimed I had reje...       0        1         0   \n",
            "After a week of hassle with their customer serv...       0        0         0   \n",
            " I won't be using them again.                            0        0         0   \n",
            "There are plenty of suppliers who may charge a ...       1        0         0   \n",
            "There goes several hours of my life I'll never ...       0        0         0   \n",
            "\n",
            "                                                    customer  day  deliver  \\\n",
            "I ordered over £1000 worth of computer equipmen...         0    1        0   \n",
            " It never turned up and they claimed I had reje...         0    0        0   \n",
            "After a week of hassle with their customer serv...         1    0        0   \n",
            " I won't be using them again.                              0    0        0   \n",
            "There are plenty of suppliers who may charge a ...         0    0        1   \n",
            "There goes several hours of my life I'll never ...         0    0        0   \n",
            "\n",
            "                                                    ...  rejected  service  \\\n",
            "I ordered over £1000 worth of computer equipmen...  ...         0        0   \n",
            " It never turned up and they claimed I had reje...  ...         1        0   \n",
            "After a week of hassle with their customer serv...  ...         0        1   \n",
            " I won't be using them again.                       ...         0        0   \n",
            "There are plenty of suppliers who may charge a ...  ...         0        0   \n",
            "There goes several hours of my life I'll never ...  ...         0        0   \n",
            "\n",
            "                                                    stick  suppliers  told  \\\n",
            "I ordered over £1000 worth of computer equipmen...      0          0     0   \n",
            " It never turned up and they claimed I had reje...      0          0     0   \n",
            "After a week of hassle with their customer serv...      1          0     1   \n",
            " I won't be using them again.                           0          0     0   \n",
            "There are plenty of suppliers who may charge a ...      0          1     0   \n",
            "There goes several hours of my life I'll never ...      0          0     0   \n",
            "\n",
            "                                                    turned  using  week  won  \\\n",
            "I ordered over £1000 worth of computer equipmen...       0      0     0    0   \n",
            " It never turned up and they claimed I had reje...       1      0     0    0   \n",
            "After a week of hassle with their customer serv...       0      0     1    0   \n",
            " I won't be using them again.                            0      1     0    1   \n",
            "There are plenty of suppliers who may charge a ...       0      0     0    0   \n",
            "There goes several hours of my life I'll never ...       0      0     0    0   \n",
            "\n",
            "                                                    worth  \n",
            "I ordered over £1000 worth of computer equipmen...      1  \n",
            " It never turned up and they claimed I had reje...      0  \n",
            "After a week of hassle with their customer serv...      0  \n",
            " I won't be using them again.                           0  \n",
            "There are plenty of suppliers who may charge a ...      0  \n",
            "There goes several hours of my life I'll never ...      0  \n",
            "\n",
            "[6 rows x 38 columns]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}